{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:nlp_rnn] *",
      "language": "python",
      "name": "conda-env-nlp_rnn-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Text_Generator.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAlyAaFuByba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIyH86HsBybg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwycbEZWBybj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9360938b-b70d-4821-abf1-ded9f0612cd6"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpJJ2BuGBybm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### ENCODING + PREPROCESSING ###"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ienz0M0iBybr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Figure out number of unique characters in our vocab\n",
        "# Then map from unique characters to indices\n",
        "# Turn inital vocab into a list, going from index to letter\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "#mapping uniques characyers to indices\n",
        "charactersToIdx = {u:i for i, u in enumerate(vocab)}\n",
        "idxToCharacters = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "    return np.array([charactersToIdx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCtO_a_4Bybu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1be011c7-6aae-4ead-b9e1-d93e69872f30"
      },
      "source": [
        "# testing how it has worked\n",
        "\n",
        "print(\"Text: \", text[:15])\n",
        "print(\"Encoded: \", text_to_int(text[:15]))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text:  First Citizen:\n",
            "\n",
            "Encoded:  [18 47 56 57 58  1 15 47 58 47 64 43 52 10  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6aZtYxHBybx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4b9727ee-35aa-4a6e-c50b-44e2e04d1c9b"
      },
      "source": [
        "# Converting numeric values to text â€” maybe needed later\n",
        "\n",
        "def int_to_text(integers):\n",
        "    try:\n",
        "        integers = integers.numpy()\n",
        "    except:\n",
        "        pass\n",
        "    return ''.join(idxToCharacters[integers])\n",
        "    \n",
        "print(int_to_text(text_as_int[:15]))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVCY4qzYByb1",
        "colab_type": "text"
      },
      "source": [
        "### Creating training examples from text file\n",
        "\n",
        "E.g. Input: 'Hell' and resulting output: 'ello'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCtbtzSeByb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence_length = 100\n",
        "examples_per_epoch = len(text)//(sequence_length + 1)\n",
        "\n",
        "# COnverts our string dataset into characters. Allows us to have a stream of characters. \n",
        "# Will contain 1.1 million characters\n",
        "character_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in_BdMONByb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = character_dataset.batch(sequence_length+1, drop_remainder=True)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNcUeE8WByb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits inputs and outputs\n",
        "\n",
        "def split_input_target(input):\n",
        "    input_text = input[:-1] #hell\n",
        "    target_text = input[1:] #ello\n",
        "    \n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "# Applies function to every entry in the characters created above"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "143P3UuVByb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "14bc969a-c7fb-422d-8fb1-bfe1af904a3b"
      },
      "source": [
        "# Checking some examples\n",
        "\n",
        "for x, y in dataset.take(2):\n",
        "    print('\\n\\nTESTING\\n\"')\n",
        "    print('INPUT')\n",
        "    print(int_to_text(x))\n",
        "    print(\"\\nOUT\\n\")\n",
        "    print(int_to_text(y))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "TESTING\n",
            "\"\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUT\n",
            "\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "TESTING\n",
            "\"\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUT\n",
            "\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syi16xCqBycB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making trainin batches\n",
        "# Feed model 64 batches of data at a time\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxcMY2e-BycE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### BUILDING MODEL ###"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiLU3n-1BycH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "008c008d-6a99-47a6-b700-3e2fe932e4f7"
      },
      "source": [
        "# Writing a function to return to us a built model\n",
        "\n",
        "def build_model(vocab_size, embedding_dimensions, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dimensions, batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "        # want final layer to have # of nodes = # of words in vocab. Each node represents a probability distribtion\n",
        "        # that that character comes next\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t205oet7BycK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input to our model has length 64 (batches of 64 examples, each a sequence of length 100)\n",
        "# Ouput in the probability of each word in the entire vocabulary from occuring"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz4NoEOzBycN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "551ac82d-5b74-430b-b7ee-49c21af38c87"
      },
      "source": [
        "# Looking at sample input and output of model\n",
        "\n",
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "    example_predictions = model(input_example_batch)\n",
        "    print(example_predictions.shape, '# (batch_size, seq_len, vocab_size')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, seq_len, vocab_size\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2aUijqPBycQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "056ab2ac-b32b-430e-c9f2-cdc417d1909a"
      },
      "source": [
        "# Prediction is an array of 64 arrays (output shape 1 in dense layer)\n",
        "\n",
        "print(len(example_predictions))\n",
        "print(example_predictions)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-1.21543324e-03 -3.15926992e-03  1.03446969e-03 ... -2.36298982e-03\n",
            "   -4.13387083e-04  1.67113368e-03]\n",
            "  [-8.87477247e-04 -1.01622129e-02  2.39519170e-03 ...  2.41522957e-03\n",
            "    3.03720264e-03  4.39212751e-03]\n",
            "  [-2.28902511e-03 -8.93780403e-03 -2.48057535e-03 ...  5.60427958e-04\n",
            "    6.31489325e-03  5.27122756e-03]\n",
            "  ...\n",
            "  [ 6.01371098e-03  8.46289750e-03 -8.05195421e-03 ...  7.75235053e-03\n",
            "   -3.79582541e-03 -1.38400099e-03]\n",
            "  [ 8.82256497e-03  8.56757350e-03 -1.10681374e-02 ...  9.23106726e-03\n",
            "   -1.48668594e-03 -1.23479648e-03]\n",
            "  [ 6.18119678e-03  9.34889633e-03 -6.03795890e-03 ...  2.14844989e-03\n",
            "   -4.69520828e-03 -1.70410518e-03]]\n",
            "\n",
            " [[-6.52959710e-03 -6.66230218e-03  9.19432670e-04 ... -2.42957976e-04\n",
            "   -2.70727975e-03 -7.22856203e-04]\n",
            "  [-1.01877246e-02 -3.78673244e-03  2.93302728e-04 ...  4.47414629e-03\n",
            "    9.51103459e-04  4.23900643e-03]\n",
            "  [-4.71330294e-03 -5.24641946e-03 -4.60825535e-03 ...  4.68796259e-03\n",
            "   -4.90565738e-03  5.78734512e-03]\n",
            "  ...\n",
            "  [ 4.14730283e-03 -1.52342916e-02 -5.63092250e-03 ...  1.52441429e-03\n",
            "   -1.18303513e-02 -1.61245291e-03]\n",
            "  [ 1.56397626e-04 -1.05961785e-02  1.93760861e-04 ...  6.87871175e-03\n",
            "   -6.74599130e-03  7.00351503e-03]\n",
            "  [ 3.99790984e-03 -9.51877329e-03 -6.57772366e-03 ...  4.82086558e-03\n",
            "   -1.15580969e-02  6.35475665e-03]]\n",
            "\n",
            " [[-5.51170204e-03  1.02385110e-03 -7.55839515e-04 ...  3.22881900e-03\n",
            "    2.17687571e-03  4.35630232e-03]\n",
            "  [-1.33209745e-03  3.22691980e-03 -4.33528516e-03 ...  8.40281509e-03\n",
            "    4.95627848e-03  4.09278739e-03]\n",
            "  [-2.01683235e-03 -4.71582316e-04 -2.01524701e-03 ...  2.92420760e-03\n",
            "    3.32296127e-03  4.96278284e-03]\n",
            "  ...\n",
            "  [-4.02794359e-03 -6.62509864e-03 -4.00056923e-03 ... -1.46893563e-03\n",
            "   -1.00359088e-02 -3.70036927e-03]\n",
            "  [-2.32045446e-03 -1.22347986e-02 -1.34587893e-03 ...  4.04115580e-03\n",
            "   -4.00142139e-03  1.52866123e-03]\n",
            "  [-2.30485504e-03 -2.15868093e-03 -6.28090743e-03 ...  7.03787431e-03\n",
            "   -5.07090893e-03  3.28718219e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 2.17434531e-03 -8.09754769e-04  1.53899542e-03 ... -2.94812839e-04\n",
            "    2.80548935e-04  2.79247249e-03]\n",
            "  [ 4.88832779e-03 -3.68393608e-03 -4.66835219e-03 ... -1.38183543e-03\n",
            "   -5.47104655e-03  3.77130602e-03]\n",
            "  [ 3.10186110e-03  3.19835288e-03 -3.67163855e-04 ... -8.06763303e-03\n",
            "   -6.42399769e-03  5.03248489e-03]\n",
            "  ...\n",
            "  [ 2.52081151e-03 -2.13705283e-03  2.14527617e-03 ... -4.19102842e-03\n",
            "   -5.33427764e-03 -1.26108425e-02]\n",
            "  [-7.85846903e-04 -7.35162757e-03 -6.75107061e-04 ... -4.34145425e-03\n",
            "   -6.05392735e-04 -1.35170985e-02]\n",
            "  [-5.74941002e-03 -3.58295836e-03 -2.16675708e-05 ... -6.22274727e-03\n",
            "    4.09739325e-03 -1.14543326e-02]]\n",
            "\n",
            " [[ 3.90775938e-04 -4.94849449e-03 -4.59252018e-03 ...  5.29382785e-04\n",
            "   -4.55072010e-03 -1.03260088e-03]\n",
            "  [ 1.34140369e-03 -1.11835087e-02 -2.05056928e-03 ...  6.38597365e-03\n",
            "   -5.61571323e-05  2.16714363e-03]\n",
            "  [ 5.11304429e-03 -5.11413813e-03 -7.31052086e-03 ...  1.09251402e-02\n",
            "    2.69736047e-03  1.10284670e-03]\n",
            "  ...\n",
            "  [-5.33857383e-03  5.98944398e-03 -4.29946044e-03 ... -7.00640073e-03\n",
            "    5.22375898e-03 -1.88159567e-04]\n",
            "  [-7.05606630e-03  4.50254977e-03  8.03417701e-04 ... -1.66584365e-03\n",
            "    6.44193031e-03  6.41658530e-03]\n",
            "  [-1.68082805e-03 -2.40307511e-03 -2.60488852e-03 ... -2.41985102e-03\n",
            "    5.69478981e-03 -3.66823486e-04]]\n",
            "\n",
            " [[-6.52959710e-03 -6.66230218e-03  9.19432670e-04 ... -2.42957976e-04\n",
            "   -2.70727975e-03 -7.22856203e-04]\n",
            "  [-7.77396513e-03 -4.24186373e-03  5.03612449e-03 ...  7.11585442e-03\n",
            "    1.30698027e-03  8.02394282e-03]\n",
            "  [-2.18680990e-03 -8.24362226e-03  8.44474765e-04 ...  7.06175156e-03\n",
            "    2.48832093e-03  2.45988159e-03]\n",
            "  ...\n",
            "  [ 9.14039277e-03  1.68261596e-03 -6.59476919e-03 ...  5.73750492e-03\n",
            "   -1.17047615e-02  4.01187222e-03]\n",
            "  [ 6.68889098e-03  6.91510912e-04 -2.67284364e-03 ...  6.05455646e-03\n",
            "   -5.43717481e-03 -1.33926689e-03]\n",
            "  [ 9.72436462e-03  4.37313411e-03 -1.09555887e-03 ...  1.74585986e-03\n",
            "   -1.45422830e-03 -1.32649613e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQjcG7e2BycS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "71ba74e5-a41e-44c7-f0a8-4eab3ec388f0"
      },
      "source": [
        "# Testing just one prediction in an untrained model\n",
        "\n",
        "pred = example_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-0.00121543 -0.00315927  0.00103447 ... -0.00236299 -0.00041339\n",
            "   0.00167113]\n",
            " [-0.00088748 -0.01016221  0.00239519 ...  0.00241523  0.0030372\n",
            "   0.00439213]\n",
            " [-0.00228903 -0.0089378  -0.00248058 ...  0.00056043  0.00631489\n",
            "   0.00527123]\n",
            " ...\n",
            " [ 0.00601371  0.0084629  -0.00805195 ...  0.00775235 -0.00379583\n",
            "  -0.001384  ]\n",
            " [ 0.00882256  0.00856757 -0.01106814 ...  0.00923107 -0.00148669\n",
            "  -0.0012348 ]\n",
            " [ 0.0061812   0.0093489  -0.00603796 ...  0.00214845 -0.00469521\n",
            "  -0.00170411]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAZhALuwBycU",
        "colab_type": "text"
      },
      "source": [
        "Above returns a 2d array of length 200. Each interior array is the prediction for the next character at each time step. I.e. for every single training example, # of outputs = len(of that training example). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe4b63sUBycV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "9604dba2-9d74-468b-f2bc-acd33e056935"
      },
      "source": [
        "# Breaking the above pred down into prediction at the first timestep.\n",
        "# Each of the 65 values represent the probability of each character occuring next. \n",
        "\n",
        "\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-1.2154332e-03 -3.1592699e-03  1.0344697e-03 -1.7614500e-03\n",
            "  2.2874640e-03  6.9755595e-03 -8.4607652e-04  1.3689753e-03\n",
            " -1.6421891e-03  5.1368936e-03 -1.2422957e-03 -1.1372651e-03\n",
            "  2.7795770e-04  1.0755707e-03  2.4355105e-03  3.4512414e-03\n",
            "  1.1145249e-03 -1.9000007e-03  8.3720288e-04 -3.6827221e-03\n",
            " -8.6951721e-03  1.1118244e-06 -8.4905419e-04  2.8053022e-03\n",
            "  1.9380373e-04  2.4720279e-03  2.3212014e-03  2.4788612e-03\n",
            "  1.5750899e-03  1.1911441e-03 -1.6734493e-03 -1.5311562e-04\n",
            "  4.0316242e-03 -4.9043931e-03  2.4905037e-03  8.8540910e-06\n",
            " -8.4353982e-05  8.4996247e-04  2.9782462e-03  2.5591212e-03\n",
            "  2.7932512e-04 -3.4848810e-03 -6.0216349e-04  1.0158742e-02\n",
            "  6.7483415e-03 -1.2463115e-03 -1.3220469e-03  4.6794396e-04\n",
            " -3.4195411e-03  1.4551366e-03 -2.9187398e-03 -2.3121093e-03\n",
            " -4.4043077e-04 -9.6706452e-04  1.7461403e-03 -1.6683291e-03\n",
            "  5.1068113e-04  2.6224845e-03  6.3147617e-04 -3.4490749e-03\n",
            "  4.6555325e-03 -3.9357803e-04 -2.3629898e-03 -4.1338708e-04\n",
            "  1.6711337e-03], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia92cRhJBycY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e82c068-8df1-4f51-fd3d-8f7ce7f1baf5"
      },
      "source": [
        "# Sample the categorical distribution to determine a predicted character. (based on prob)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# reshaping + converting ints to characters\n",
        "\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_characters = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_characters\n",
        "# Below is hwat model predicts for training sequence 1"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eUw!LplXY&itpOo-TcfRJwjDNgxHMDMtB:l.wBuEV vz,NTaNna!Mme;GC-SbjNrnSejG?Xs-e-D3S?E\\nUBwP;SPhxtnWTyo lIq'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0aZybueBycb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### LOSS fn ###\n",
        "\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvSC0RCXBycd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### COMPILING Model ###"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIhB8WEEBycf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6zIMfNwByci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### CHECKPOINTS ### to save checkpoints during training"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE0WKmIFByck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "#naming file\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"cpoint_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tHF8LAiBycq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### TRAINING ###"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ALEkD_bByct",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e3227d40-c910-494e-d2e2-32efa87fabf5"
      },
      "source": [
        "history = model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "172/172 [==============================] - 27s 157ms/step - loss: 2.5884\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 27s 159ms/step - loss: 1.8881\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 1.6388\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 1.5043\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 27s 159ms/step - loss: 1.4231\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 1.3673\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 1.3231\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 1.2834\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 1.2468\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 1.2118\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 1.1766\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 1.1400\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 1.1013\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 1.0632\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 1.0236\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 27s 159ms/step - loss: 0.9832\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 0.9421\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.9026\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 0.8619\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 0.8245\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.7866\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 0.7553\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.7227\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 28s 161ms/step - loss: 0.6943\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 0.6676\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.6421\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 28s 161ms/step - loss: 0.6222\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 28s 161ms/step - loss: 0.6013\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.5831\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.5675\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.5539\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 0.5389\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.5269\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.5163\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.5073\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.4960\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.4894\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 0.4817\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 27s 160ms/step - loss: 0.4760\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 28s 160ms/step - loss: 0.4703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bdxcdInE9vR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### LOADING IN THE MODEL ###\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Io46LvBycx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find latest checkpoint thats stored model weights\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhUe36TZFkCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Num chars to generate\n",
        "  num_generate = 300\n",
        "\n",
        "  #VEctorizing and converting start string to ints\n",
        "  input_eval = [charactersToIdx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  generated_text = []\n",
        "\n",
        "  # Where low temp = more predictable text, higher text = more surprising text\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # categorical distribution\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "    # pass predicted char as next input to the model + previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    generated_text.append(idxToCharacters[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(generated_text))\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1dXMPGeFqHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "116f6c25-ccac-4728-efdf-beb508da84c3"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "out = generate_text(model, inp)\n",
        "#next = out.split(' ')[1]\n",
        "\n",
        "print(out)\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: enter\n",
            "enter\n",
            "To meet upon the town and honour to appeard!\n",
            "You must to pardon me, and look con?\n",
            "\n",
            "Third Citizen:\n",
            "It will not stay with me already, sir,\n",
            "But my arrival and my weal ornet, where lies herbs,\n",
            "Imabilitable and false; I believe me.\n",
            "Throw up your king, But was foul water for all this land\n",
            "As 'longeth too\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d__-rJRvSETR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}